{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kstest, mannwhitneyu\n",
    "from scipy.stats import ks_2samp\n",
    "import itertools\n",
    "# import shap\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "met_file_path = \"C:/Users/shrra/Downloads/merged_n586_Alasdair_May21_concatenated_renamed.csv\"\n",
    "met1_df = pd.read_csv(met_file_path)\n",
    "# print(met1_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met1_df_T  = met1_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "met1_df_T.reset_index(inplace=True)\n",
    "\n",
    "# Set the first row as the header\n",
    "met1_df_T.columns = met1_df_T.iloc[0]\n",
    "met1_df_T = met1_df_T[1:]\n",
    "\n",
    "# Reset the index again (if you want to have a clean numeric index)\n",
    "met1_df_T.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the newly created index column (if needed)\n",
    "met1_df_T.rename(columns={met1_df_T.columns[0]: 'ID'}, inplace=True)\n",
    "\n",
    "\n",
    "# print(met1_df_T.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met1_df_T['ID'] = met1_df_T['ID'].str.replace('-S.*', '', regex=True)\n",
    "# print(len(met1_df_T))\n",
    "# print(met1_df_T.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_file_path = \"C:/Users/shrra/Downloads/MASH_biomarkers_clinicaldata_notimputed_May30_HH.csv\"\n",
    "mash_df = pd.read_csv(mas_file_path)\n",
    "mash_df['MASH'] = mash_df['MASH'].map({'MASH': 1, 'non-MASH': 0})\n",
    "# print(mash_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = mash_df[['ID', 'MASH']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df['ID'] = id_df['ID'].str.replace('5-00', '')\n",
    "id_df['ID'] = id_df['ID'].str.replace('5-0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = id_df[['ID']]\n",
    "df2 = met1_df_T[['ID']]\n",
    "\n",
    "\n",
    "set1 = set(df1['ID'])\n",
    "set2 = set(df2['ID'])\n",
    "\n",
    "# Find values in df1['ID'] that are not in df2['ID']\n",
    "missing_in_df2 = set1 - set2\n",
    "\n",
    "# Find values in df2['ID'] that are not in df1['ID']\n",
    "missing_in_df1 = set2 - set1\n",
    "\n",
    "# # Display the missing values\n",
    "# print(\"\\nID Values in mash file that are not in metab file\")\n",
    "# print(missing_in_df2)\n",
    "# print(len(missing_in_df2))\n",
    "# print(\"\\nID Values in metab file that are not in mash file\")\n",
    "# print(missing_in_df1)\n",
    "# print(len(missing_in_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_meta_df = pd.merge(id_df, met1_df_T, on=['ID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KSTest_df = Updated_meta_df\n",
    "\n",
    "\n",
    "excluded_cols = [\"ID\"]\n",
    "KSTest_df= KSTest_df.drop(columns=excluded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zeros = (KSTest_df == 0).any(axis=1).sum()\n",
    "\n",
    "print(f\"Number of records with at least one 0 value: {count_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill 0 \n",
    "\n",
    "columns_to_fill = KSTest_df.columns[KSTest_df.columns != 'MASH']\n",
    "\n",
    "# Replace 0s with NaNs in the selected columns\n",
    "KSTest_df[columns_to_fill] = KSTest_df[columns_to_fill].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = KSTest_df.isna().sum()\n",
    "\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_counts)\n",
    "non_nan_columns_count = KSTest_df.notna().all().sum()\n",
    "\n",
    "print(f\"Number of columns without any NaN values: {non_nan_columns_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from scipy.stats import ks_2samp\n",
    "\n",
    "# # Assuming KSTest_df is your DataFrame\n",
    "# target_column = 'MASH'\n",
    "\n",
    "# # Calculate p-values for each feature in advance and store in a dictionary\n",
    "# p_values_dict = {}\n",
    "# for feature in KSTest_df.columns:\n",
    "#     if feature != target_column:\n",
    "#         Controls = KSTest_df[KSTest_df[target_column] == 0][feature].dropna()\n",
    "#         MASH = KSTest_df[KSTest_df[target_column] == 1][feature].dropna()\n",
    "\n",
    "#         # Ensure the columns are numeric\n",
    "#         Controls = pd.to_numeric(Controls, errors='coerce')\n",
    "#         MASH = pd.to_numeric(MASH, errors='coerce')\n",
    "\n",
    "#         _, KS_p_value = ks_2samp(Controls, MASH)\n",
    "#         p_values_dict[feature] = KS_p_value\n",
    "\n",
    "# # Convert the dictionary to a DataFrame for better visualization\n",
    "# ranking_df = pd.DataFrame(list(p_values_dict.items()), columns=['feature', 'KS p-value'])\n",
    "\n",
    "# # Print the p-values DataFrame\n",
    "# # print(\"\\nP-Values DataFrame:\\n\", ranking_df)\n",
    "\n",
    "# # Calculate correlation matrix\n",
    "# corr_matrix = KSTest_df.drop(target_column, axis=1).corr().abs()\n",
    "\n",
    "# # Set threshold for high correlation\n",
    "# corr_threshold_low = 0.5\n",
    "\n",
    "# # Dictionary to store highly correlated features for each feature\n",
    "# highly_correlated_features = {}\n",
    "\n",
    "# # Loop through the features and find highly correlated features\n",
    "# for feature in corr_matrix.columns:\n",
    "#     highly_corr = corr_matrix[(corr_matrix[feature] > corr_threshold_low)][feature].index.tolist()\n",
    "#     if highly_corr:\n",
    "#         highly_correlated_features[feature] = highly_corr\n",
    "\n",
    "# # Perform KS test for each group and keep the feature with the best p-value\n",
    "# selected_features = set()\n",
    "# ks_test_results = {}\n",
    "\n",
    "# # DataFrame to store selected features and their p-values\n",
    "# selected_features_df = pd.DataFrame(columns=['Group', 'Selected_Feature', 'p-value'])\n",
    "\n",
    "# # Set to keep track of already selected features\n",
    "# excluded_features = set()\n",
    "\n",
    "# for feature, correlated in highly_correlated_features.items():\n",
    "#     # Filter out already selected features\n",
    "#     candidates = [col for col in correlated + [feature] if col not in excluded_features]\n",
    "#     if not candidates:\n",
    "#         continue\n",
    "#     # Use precomputed p-values from the dictionary\n",
    "#     p_values = {col: p_values_dict[col] for col in candidates}\n",
    "#     ks_test_results[feature] = p_values\n",
    "#     best_feature = min(p_values, key=p_values.get)\n",
    "#     selected_features.add(best_feature)\n",
    "#     excluded_features.update(candidates)\n",
    "    \n",
    "#     # Add the selected feature and its p-value to the DataFrame\n",
    "#     new_row = pd.DataFrame({'Group': [feature], 'Selected_Feature': [best_feature], 'p-value': [p_values[best_feature]]})\n",
    "#     selected_features_df = pd.concat([selected_features_df, new_row], ignore_index=True)\n",
    "\n",
    "# # Final set of features\n",
    "# final_selected_features = list(selected_features)\n",
    "\n",
    "# # Display the DataFrame of selected features and their p-values\n",
    "# # print(\"\\nThis df has Selected Features and Their p-values:\\n\", selected_features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Assuming KSTest_df is your DataFrame\n",
    "target_column = 'MASH'\n",
    "\n",
    "# Calculate p-values for each feature in advance and store in a dictionary\n",
    "p_values_dict = {}\n",
    "for feature in KSTest_df.columns:\n",
    "    if feature != target_column:\n",
    "        # Extract Controls and MASH groups\n",
    "        Controls = pd.to_numeric(KSTest_df[KSTest_df[target_column] == 0][feature], errors='coerce')\n",
    "        MASH = pd.to_numeric(KSTest_df[KSTest_df[target_column] == 1][feature], errors='coerce')\n",
    "\n",
    "        # Perform KS test with NaN handling\n",
    "        _, KS_p_value = ks_2samp(Controls, MASH, nan_policy='omit')\n",
    "        p_values_dict[feature] = KS_p_value\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "ranking_df = pd.DataFrame(list(p_values_dict.items()), columns=['feature', 'KS p-value'])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = KSTest_df.drop(target_column, axis=1).corr().abs()\n",
    "\n",
    "# Set threshold for high correlation\n",
    "corr_threshold_low = 0.5\n",
    "\n",
    "# Dictionary to store highly correlated features for each feature\n",
    "highly_correlated_features = {}\n",
    "\n",
    "# Loop through the features and find highly correlated features\n",
    "for feature in corr_matrix.columns:\n",
    "    highly_corr = corr_matrix[(corr_matrix[feature] > corr_threshold_low)][feature].index.tolist()\n",
    "    if highly_corr:\n",
    "        highly_correlated_features[feature] = highly_corr\n",
    "\n",
    "# Perform KS test for each group and keep the feature with the best p-value\n",
    "selected_features = set()\n",
    "ks_test_results = {}\n",
    "\n",
    "# DataFrame to store selected features and their p-values\n",
    "selected_features_df = pd.DataFrame(columns=['Group', 'Selected_Feature', 'p-value'])\n",
    "\n",
    "# Set to keep track of already selected features\n",
    "excluded_features = set()\n",
    "\n",
    "for feature, correlated in highly_correlated_features.items():\n",
    "    # Filter out already selected features\n",
    "    candidates = [col for col in correlated + [feature] if col not in excluded_features]\n",
    "    if not candidates:\n",
    "        continue\n",
    "    # Use precomputed p-values from the dictionary\n",
    "    p_values = {col: p_values_dict[col] for col in candidates}\n",
    "    ks_test_results[feature] = p_values\n",
    "    best_feature = min(p_values, key=p_values.get)\n",
    "    selected_features.add(best_feature)\n",
    "    excluded_features.update(candidates)\n",
    "    \n",
    "    # Add the selected feature and its p-value to the DataFrame\n",
    "    new_row = pd.DataFrame({'Group': [feature], 'Selected_Feature': [best_feature], 'p-value': [p_values[best_feature]]})\n",
    "    selected_features_df = pd.concat([selected_features_df, new_row], ignore_index=True)\n",
    "\n",
    "# Final set of features\n",
    "final_selected_features = list(selected_features)\n",
    "\n",
    "# Display the DataFrame of selected features and their p-values\n",
    "print(selected_features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_selected_features))\n",
    "check = set(final_selected_features)\n",
    "print(\"So unique would be\")\n",
    "print(len(check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming selected_features_df is your original DataFrame\n",
    "# threshold = 0.05 / 3759\n",
    "threshold = 0.05\n",
    "\n",
    "# Initialize count and a list to store the rows that meet the criteria\n",
    "count = 0\n",
    "filtered_rows = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame\n",
    "for index, row in selected_features_df.iterrows():\n",
    "    if row['p-value'] < threshold:\n",
    "        count += 1\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "# Create a new DataFrame from the filtered rows\n",
    "Pvalue_cutoff_df = pd.DataFrame(filtered_rows)\n",
    "\n",
    "print(\"number of features under  0.05 / 3759= \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pvalue_cutoff_df.columns)\n",
    "abc =Pvalue_cutoff_df['Selected_Feature']\n",
    "abc=list(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredf_60_Mash= pd.DataFrame()\n",
    "featuredf_60_Mash['MASH'] = KSTest_df['MASH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredf_60 = KSTest_df[abc]\n",
    "\n",
    "\n",
    "# Concatenate the existing df_selected with the additional selected features\n",
    "featuredf_60 = pd.concat([featuredf_60_Mash,featuredf_60], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load data\n",
    "# data = featuredf_60\n",
    "# X = data.drop('MASH', axis=1)  # Assuming the target column is named 'target'\n",
    "# y = data['MASH']\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# # Initialize SVM model\n",
    "# svm = SVC(kernel='linear')\n",
    "\n",
    "# # Initialize RFE with SVM\n",
    "# rfe = RFE(estimator=svm, n_features_to_select=20)\n",
    "\n",
    "# # Create a pipeline\n",
    "# pipeline = Pipeline([('rfe', rfe), ('svm', svm)])\n",
    "\n",
    "# # Train model\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Get the ranking of features\n",
    "# ranking = rfe.ranking_\n",
    "# print(f\"Feature ranking: {ranking}\")\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Model accuracy with selected features: {accuracy}\")\n",
    "\n",
    "# # Selected features\n",
    "# selected_features = [feature for feature, rank in zip(data.feature_names, ranking) if rank == 1]\n",
    "# print(f\"Selected features: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data = featuredf_60\n",
    "X = data.drop('MASH', axis=1) \n",
    "y = data['MASH']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "# Initialize RFE with LightGBM\n",
    "rfe = RFE(estimator=lgbm, n_features_to_select=20, step=1)\n",
    "\n",
    "# Train RFE with LightGBM\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rfe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy with selected features: {accuracy}\")\n",
    "\n",
    "# Selected features\n",
    "lightgbm_selected_features = [feature for feature, rank in zip(X.columns, ranking) if rank == 1]\n",
    "print(f\"Selected features: {lightgbm_selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data = featuredf_60\n",
    "X = data.drop('MASH', axis=1) \n",
    "y = data['MASH']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=0)\n",
    "\n",
    "# Initialize RFE with CatBoost\n",
    "rfe = RFE(estimator=catboost, n_features_to_select=20, step=1)\n",
    "\n",
    "# Train RFE with CatBoost\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rfe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy with selected features: {accuracy}\")\n",
    "\n",
    "# Selected features\n",
    "catboost_selected_features = [feature for feature, rank in zip(X.columns, ranking) if rank == 1]\n",
    "print(f\"Selected features: {catboost_selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffeaturecsv=pd.DataFrame()\n",
    "ffeaturecsv['LightGBM features'] = lightgbm_selected_features\n",
    "ffeaturecsv['CatBoost features'] = catboost_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffeaturecsv.to_csv('C:/Users/shrra/Downloads/MASH_ML/Top20features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final 20 sel + Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_20= ['ID','MASH','119.06855_53.2', '288.18835_28.15', '239.23685_26.6', '518.3686_26.5', '173.9294_147.65', '201.04415_88.3', '204.99065_70.85', '212.05745_62.05', '242.2841_23.2', '254.0925_89.9', '265.154_28.25', '348.90945_101.85', '357.981_52.6', '365.7559_111.6', '456.06815_78.45', '506.6724_54', '534.1962_26', '564.42165_25.75', '583.29885_37.85', '646.0214_23.8']\n",
    "clin_15=['AST (U/L)', 'ALT (U/L)','GGT (U/L)','Platelet count','Female n(%)','HOMA-IR','Insulin (uU/mL)','Albumin (g/dL)','BMI z-score','TG:HDL ratio','Alk phos (U/L)','Age (yrs.)','HDL-c (mg/dL)','Creatinine (mg/dL)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metab_20df= Updated_meta_df[cat_20]\n",
    "clin_15_df = mash_df[clin_15]\n",
    "final_df = pd.DataFrame()\n",
    "final_df\n",
    "final_df = pd.concat([metab_20df,clin_15_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your dataset\n",
    "data = final_df3.+\n",
    "data = data.drop(columns='ID')\n",
    "\n",
    "\n",
    "# Define the target and features\n",
    "target = 'MASH'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, \n",
    "                           learning_rate=0.1, \n",
    "                           depth=6, \n",
    "                           loss_function='Logloss', \n",
    "                           verbose=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plot AUROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'AUROC = {auroc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding top 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data = featuredf_60\n",
    "X = data.drop('MASH', axis=1) \n",
    "y = data['MASH']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "\n",
    "# Initialize RFE with LightGBM\n",
    "rfe = RFE(estimator=lgbm, n_features_to_select=10, step=1)\n",
    "\n",
    "# Train RFE with LightGBM\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rfe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy with selected features: {accuracy}\")\n",
    "\n",
    "# Selected features\n",
    "lightgbm_selected_10_features = [feature for feature, rank in zip(X.columns, ranking) if rank == 1]\n",
    "print(f\"Selected features: {lightgbm_selected_10_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data = featuredf_60\n",
    "X = data.drop('MASH', axis=1) \n",
    "y = data['MASH']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "catboost = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=0)\n",
    "\n",
    "# Initialize RFE with CatBoost\n",
    "rfe = RFE(estimator=catboost, n_features_to_select=10, step=1)\n",
    "\n",
    "# Train RFE with CatBoost\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the ranking of features\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rfe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy with selected features: {accuracy}\")\n",
    "\n",
    "# Selected features\n",
    "catboost_selected_10features = [feature for feature, rank in zip(X.columns, ranking) if rank == 1]\n",
    "print(f\"Selected features: {catboost_selected_10features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffeatur10ecsv=pd.DataFrame()\n",
    "ffeatur10ecsv['LightGBM features'] = lightgbm_selected_10_features\n",
    "ffeatur10ecsv['CatBoost features'] = catboost_selected_10features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffeatur10ecsv.to_csv('C:/Users/shrra/Downloads/MASH_ML/Top10features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final 10 sel + Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_10= ['ID','MASH','119.06855_53.2', '518.3686_26.5', '201.04415_88.3', '212.05745_62.05', '254.0925_89.9', '265.154_28.25', '357.981_52.6', '456.06815_78.45', '506.6724_54', '646.0214_23.8']\n",
    "clin_15=['AST (U/L)', 'ALT (U/L)','GGT (U/L)','Platelet count','Female n(%)','HOMA-IR','Insulin (uU/mL)','Albumin (g/dL)','BMI z-score','TG:HDL ratio','Alk phos (U/L)','Age (yrs.)','HDL-c (mg/dL)','Creatinine (mg/dL)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metab_10df= Updated_meta_df[cat_10]\n",
    "clin_15_df = mash_df[clin_15]\n",
    "final_df = pd.DataFrame()\n",
    "final_df\n",
    "final_10df = pd.concat([metab_10df,clin_15_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your dataset\n",
    "data = final_10df\n",
    "data = data.drop(columns='ID')\n",
    "\n",
    "\n",
    "# Define the target and features\n",
    "target = 'MASH'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, \n",
    "                           learning_rate=0.1, \n",
    "                           depth=6, \n",
    "                           loss_function='Logloss', \n",
    "                           verbose=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plot AUROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'AUROC = {auroc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
